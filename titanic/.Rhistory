map("world", col="#191919", fill=TRUE, bg="#000000", lwd=0.05, xlim=xlim, ylim=ylim)
fsub <- flights[flights$airline == carriers[i],]
fsub <- fsub[order(fsub$cnt),]
maxcnt <- max(fsub$cnt)
for (j in 1:length(fsub$airline)) {
air1 <- airports[airports$iata == fsub[j,]$airport1,]
air2 <- airports[airports$iata == fsub[j,]$airport2,]
inter <- gcIntermediate(c(air1[1,]$long, air1[1,]$lat), c(air2[1,]$long, air2[1,]$lat), n=100, addStartEnd=TRUE)
colindex <- round( (fsub[j,]$cnt / maxcnt) * length(colors) )
lines(inter, col=colors[colindex], lwd=0.6)
}
dev.off()
}
install.packages('ggmap')
library(ggmap)
library(ggmap)
install.packages('ggmap')
library(ggmap)
library(ggmap)
install.packages('googleMaps')
library(ggmap)
library(maps)
library(rgeos)
install.packages('rgeos')
library(rgeos)
library(rgeos)
library(maptools)
gpclibPermit()
gcircles
worldmap
urbanareas<-fortify(simp)
library(ggmap)
install.packages("ggmap")
library(ggmap)
library(ggplot2)
library(maps)<br />
#install.packages("rgeos")
library(rgeos)
library(maptools)
library(shapefiles)
install.packages("shapefiles")
library(shapefiles)
library(shapefiles)
library(shapefiles)
library(geosphere)
library(plyr)
library(plyr)
library(sp)
urbanareasin<-readShapePoly('ne_10m_urban_areas.shp')
urbanareasin<-readShapePoly('ne_10m_urban_areas.shp')
worldmapsin<-readShapePoly('ne_10m_admin_0_countries.shp')
urbanareasin<-sf::st_read('ne_10m_urban_areas.shp')
library(sf)
install.packages('sf')
library(sf)
install.packages('sf')
library(sf)
library(sf)
install.packages('sf')
library(sf)
library(ggmap)
library(ggplot2)
library(maps)
#install.packages("rgeos")
library(rgeos)
library(maptools)
library(shapefiles)
library(geosphere)
library(plyr)
library(sp)
urbanareasin<-readShapePoly('ne_10m_urban_areas.shp')
urbanareasin<-readShapePoly('ne_10m_urban_areas.shp')
urbanareasin<-sf::st_read('ne_10m_urban_areas.shp')
worldmapsin<-readShapePoly('ne_10m_admin_0_countries.shp')
worldmapsin<-rgdal::readOGR('ne_10m_admin_0_countries.shp')
install.packages('rgdal')
library('rgdal')
install.packages('rgdal')
library('rgdal')
library('rgdal')
library('rgdal')
install.packages('rgdal',dependencies = T)
library('rgdal')
library('rgdal')
worldmap<-fortify(worldmapsin)
urbanareasin<-readShapePoly('ne_10m_urban_areas.shp')
urbanareasin<-rgdal::readOGR('ne_10m_urban_areas.shp')
library(sf)
install.packages("twitteR")
install.packages("twitteR",dependencies = T)
library('twitteR')
library('twitteR')
library(twitteR)
tweets = searchTwitter('data mining',lang='en')
install.li
install.packages('ROAuth')
install.packages('RCurl')
install.packages('bitops')
install.packages('digest')
install.packages('rjson')
install.packages("rjson")
library(twitteR)
tweets = searchTwitter('data mining',lang='en')
library(ROAuth)
tweets = searchTwitter('data mining',lang='en')
library(0OAuth)
library(OAuth)
twitteR::setup_twitter_oauth()
setup_twitter_oauth()
install.packages('bit64')
install.packages("bit64")
install.packages("bit64",dependencies = T)
install.packages("DBI")
install.packages(c("crayon", "dplyr", "gridExtra", "multcomp", "svglite"))
library(twitteR)
install.packages("bit64")
install.packages("bit64",dependencies = T)
library(twitteR)
install.packages("httr",dependencies = T)
install.packages("httr", dependencies = T)
library(httr)
library(bit64)
install.packages("bit64", dependencies = T)
library(bit64)
install.packages("bit64")
library(bit64)
library(bit64)
install.packages('DBI')
library(DBI)
library(httr)
install.packages("R6")
install.packages("R6")
library("R6")
install.
install.packages('twitteR')
library(twitteR)
setup_twitter_oauth()
detach('package:twitteR',upload=TRUE)
detach('package:twitteR',unload=TRUE)
library(twitteR)
setup_twitter_oauth(consumer_key,consumer_secret)
setup_twitter_oauth
setup_twitter_oauth()
setup_twitter_oauth()
twitteR::setup_twitter_oauth()
setup_twitter_oauth(consumer_key,consumer_secret)
library(twitteR)
library(twitteR)
library(twitteR)
setup_twitter_oauth(consumer_key,consumer_secret)
twitteR::setup_twitter_oauth()
setup_twitter_oauth()
setup_twitter_oauth(consumer_key,consumer_secret)
install.packages("twitteR")
install.packages("RCurl")
require(twitteR)
require(RCurl)
install.packages("twitteR")
names(data)=c("Class","AGE","SEX","STEROID","ANTIVIRALS","FATIGUE","MALAISE","ANOREXIA","LIVER BIG","LIVER FIRM","SPLEEN PALPABLE","SPIDERS","ASCITES","VARICES","BILIRUBIN","ALK PHOSPHATE","SGOT","ALBUMIN","PROTIME","HISTOLOGY")
data = read.table("https://archive.ics.uci.edu//ml//machine-learning-databases//hepatitis/hepatitis.data",sep = ',')
names(data)=c("Class","AGE","SEX","STEROID","ANTIVIRALS","FATIGUE","MALAISE","ANOREXIA","LIVER BIG","LIVER FIRM","SPLEEN PALPABLE","SPIDERS","ASCITES","VARICES","BILIRUBIN","ALK PHOSPHATE","SGOT","ALBUMIN","PROTIME","HISTOLOGY")
head(data)
na.strings
data = read.table("https://archive.ics.uci.edu//ml//machine-learning-databases//hepatitis/hepatitis.data",sep = ',',na.strings = 'NA')
data = read.table("https://archive.ics.uci.edu//ml//machine-learning-databases//hepatitis/hepatitis.data",sep = ',',na.strings = '?')
names(data)=c("Class","AGE","SEX","STEROID","ANTIVIRALS","FATIGUE","MALAISE","ANOREXIA","LIVER BIG","LIVER FIRM","SPLEEN PALPABLE","SPIDERS","ASCITES","VARICES","BILIRUBIN","ALK PHOSPHATE","SGOT","ALBUMIN","PROTIME","HISTOLOGY")
head(data)
data = read.table("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/Garch.csv",header=T,sep=',')
attach(data)
names(data)
?boxplot
boxplot(cd~factor(date))
data = read.table("C:\\Users\\Administrator\\Desktop\\statistical computing\\HW02\\Gapminder.csv",header=T,sep=',')
library("ggplot2")
ggplot(data,aes(x=data$continent)) + geom_bar()
p<-ggplot(data, aes(x = gdpPercap, y = lifeExp))
ggplot(data, aes(x = gdpPercap, y = lifeExp))
ggplot(data, aes(x = gdpPercap, y = lifeExp)) + geom_point()
p <- ggplot(data, aes(gdpPercap, lifeExp))
p + geom_point()
# Experiment with a different scale
p + geom_point() + scale_x_log10()
p <- p + scale_x_log10()
p + geom_point()
p + geom_point(aes(color=continent))
p + geom_point(color="blue", pch=17, size=8, alpha=1/4)
p + geom_point(aes(color=continent))
p + geom_point(aes(col=continent, shape=continent, size=lifeExp))
p + geom_point(aes(color=continent))
vote = read.table("https://www.stat.berkeley.edu/~statlabs/data/vote.data",header=T)
hist(race,xaxt='n')
attach(vote)
hist(race,xaxt='n')
axis(1,at=seq(0,5,by=1),labels=c('missing','white','hispanic','black','asian','Other'))
ls()
data = read.table("C:\\Users\\Administrator\\Desktop\\statistical computing\\HW02\\Gapminder.csv",header=T,sep=',')
data
attach(data)
names(data)
africa = subset(continent=="Africa")
continent
continent=="Africa"
africa = data[,(continent=="Africa")]
names(data)
africa = data[(continent=="Africa"),]
africa
table(data[,"continent"])
length(africa)
dim(africa)
americas = data[(continent=="Americas"),]
asia = data[(continent=="Asia"),]
europe = data[(continent=="Europe"),]
oceania = data[(continent=="Oceania"),]
unique(c(1,2,3,3,3,3))
length(c(1,2,3,4))
africa_unique = length(unique(africa))
americas_unique = length(unique(americas))
asia_unique = length(unique(asia))
europe_unique = length(unique(europe))
oceania_unique = length(unique(oceania))
africa_unique
americas_unique
asia_unique
table(data[,"continent"])
length(africa)
dim(africa)
dim(Americas)
dim(americas)
africa
africa = data[(continent=="Africa"),"Africa"]
americas = data[(continent=="Americas"),"Americas"]
asia = data[(continent=="Asia"),"Asia"]
europe = data[(continent=="Europe"),"Europe"]
oceania = data[(continent=="Oceania"),"Oceania"]
africa_unique = length(unique(africa))
americas_unique = length(unique(americas))
asia_unique = length(unique(asia))
europe_unique = length(unique(europe))
oceania_unique = length(unique(oceania))
africa_unique
americas_unique
africa
data[,"Africa"]
names(data)
africa = data[(continent=="Africa"),"continent"]
americas = data[(continent=="Americas"),"continent"]
asia = data[(continent=="Asia"),"continent"]
europe = data[(continent=="Europe"),"continent"]
oceania = data[(continent=="Oceania"),"continent"]
africa
africa_unique = length(unique(africa))
americas_unique = length(unique(americas))
asia_unique = length(unique(asia))
europe_unique = length(unique(europe))
oceania_unique = length(unique(oceania))
africa_unique
americas_unique
asia_unique
europe_unique
oceania_unique
names(data)
> africa = data[(continent=="Africa"),"country"]
> americas = data[(continent=="Americas"),"country"]
> asia = data[(continent=="Asia"),"country"]
> europe = data[(continent=="Europe"),"country"]
> oceania = data[(continent=="Oceania"),"country"]
africa = data[(continent=="Africa"),"country"]
americas = data[(continent=="Americas"),"country"]
asia = data[(continent=="Asia"),"country"]
europe = data[(continent=="Europe"),"country"]
oceania = data[(continent=="Oceania"),"country"]
africa
africa_unique = length(unique(africa))
americas_unique = length(unique(americas))
asia_unique = length(unique(asia))
europe_unique = length(unique(europe))
oceania_unique = length(unique(oceania))
africa_unique
americas_unique
asia_unique
europe_unique
oceania_unique
x = sample(c(1,2),10)
x = sample(c(1,2),1)
x
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),1)
sample(c(1,2),10,replace=T)
x = sample(c(1,2),10,replace=T)
x
table(x)
t = table(x)
t
t(1)
t(1,1)
t(12)
t(2)
sample.space <- c(0,1)
sample(sample.space,size=N,replace=TRUE)
sample(sample.space,size=10,replace=TRUE)
sample(sample.space,size=10,replace=TRUE,prob=c(0.5,0.5))
sample(sample.space,size=10,replace=TRUE,prob=c(0.5,0.5,0.5))
sample(sample.space,size=10,replace=TRUE,prob=c(0.5,0.5,0.5))
sample(c('A','B','C'),size=10,replace=TRUE,prob=c(0.5,0.5,0.5))
sample(c('A','B','C'),size=10,replace=TRUE,prob=c(0.5,0.5,1))
sample(c('A','B','C'),size=10,replace=TRUE,prob=c(0.5,0.5,10))
sample(c('A','B','C'),size=10,replace=TRUE,prob=c(0.5,0.5,10))
prod(5:3)
choose(5,3)
dunif(x,min=0,max=1,log=FALSE)
rnorm(5)
hist(rnorm(500000))
hist(rnorm(500000))
?hist
hist(rnorm(500000),breaks = 50)
hist(rnorm(500000),breaks = 500)
hist(rnorm(500000),breaks = 100)
hist(pnorm(500000),breaks = 100)
>curve(pnorm(x),-3,3, main="CDF of Standard Normal Distribution")
curve(pnorm(x),-3,3, main="CDF of Standard Normal Distribution")
curve(pnorm(c(1,2,3,4,5)),-3,3, main="CDF of Standard Normal Distribution")
curve(pnorm(c(1,1,1,1,2)),-3,3, main="CDF of Standard Normal Distribution")
curve(y,-3,3, main="CDF of Standard Normal Distribution")
curve(pnorm(x),-3,3, main="CDF of Standard Normal Distribution")
x
curve(pnorm(x),-3,3, main="CDF of Standard Normal Distribution")
plot(pnorm(x))
pnorm(x)
curve(pnorm(x),-4,4, main="CDF of Standard Normal Distribution")
pnorm(-1)
pnorm(0)
curve(dnorm,-3,3)
lines(qnorm(0.9),dnorm(qnorm(0.9)),type="h", col="red")
qnorm(0.9)
pnorm(2)
rnorm(2)
rnorm(10)
pnorm(10)
dnorm(10)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
xbar1=numeric(1000)
for (i in 1:1000){x=runif(1);xbar1[i]=mean(x)}
xbar1
hist(xbar1,col="green",main="CLT for uniform n=1")
xbar2=numeric(1000)
for (i in 1:1000){x=runif(50);xbar2[i]=mean(x)}
hist(xbar2,col="blue",main="CLT for uniform n=50")
curve(dt(x,5),-3,3)
dt(x,5)
dt
x.norm<-rnorm(n=200,m=10,sd=2)
x.norm
hist(x.norm,main="Histogram of observed data")
plot(density(x.norm),main="Density estimate of data")
plot(ecdf(x.norm),main="Empirical CDF")
z.norm<-(x.norm-mean(x.norm))/sd(x.norm)
qqnorm(z.norm)
z.norm
abline(0,1)
abline(0,2)
abline(0,1)
unif50=runif(50)
unif100=runif(100)
norm50=rnorm(50)
norm100=rnorm(100)
lognorm50=exp(rnorm(50))
lognorm100=exp(rnorm(100))
par(mfrow=c(2,3))
plot(ecdf(unif50),pch="16")
plot(ecdf(unif100),pch="17")
plot(ecdf(norm50),pch="18")
plot(ecdf(norm100),pch="19")
plot(ecdf(lognorm50),pch="20")
plot(ecdf(lognorm100),pch="21")
data = read.csv("C:\\Users\\Administrator\\Desktop\\statistical computing\\lab08\\nyc-elevators.csv")
head(data)
names(data)
attach(data)
data1 = subset(data,Device.Status=="A")
data1
head(data1)
data1 = subset(data,Device.Status=='A'&Borough=='Manhattan')
dim(data1)
x = rnorm(500)
x
hist(x)
hist(x)
dnorm(x)
dnorm(x)
curve(dnorm(x),-4,4)
rnorm(100,5,8)
N = rnorm(100)
N
qqnorm(N)
abline(0,1)
U = runif(100)
U
qqnorm(U)
dt(5)
dt(5,5)
rt(100,5)
qqplot(rt(100,5))
T = rt(100,5)
qqplot(T)
N
T
qqnorm(T)
E = rexp(100,1)
qqnorm(E)
plot(0,0,type="n",xlim=c(0,1),ylim=c(0,13.5),
xlab="Density", ylab="f(x)")
m=500; a=0; b=1
n=2
res<- mean(runif(n, a, b))
res
res
for ( i in 1:m) res[i]<- mean(runif(n, a, b))
lines(density(res),lwd=2)
curve(runif(n,a,b))
plot(runif(n,a,b))
qqnorm(runif(n,a,b))
hist(runif(n,a,b))
runif(n,a,b)
n = 5
res<- mean(runif(n, a, b))
res
punif(n,a,b)
qunif(n,a,b)
dunif(n,a,b)
res<- mean(runif(n, a, b))
res
res<- mean(runif(n, a, b))
res<- mean(runif(n, a, b))
res
for ( i in 1:m) res[i]<- mean(runif(n, a, b))
lines( density(res), lwd=2)
n = 100
for ( i in 1:m) res[i]<- mean(runif(n, a, b))
lines( density(res), lwd=2)
m = 50; n=20; p = .5
phat = rbinom(m,n,p)/n
phat
?rbinom
runif(10,min=0,max=1)
curve(runif(10,min=0,max=1))
curve(runif(x,min=0,max=1))
curve(runif(x,min=0,max=100))
curve(runif(x,min=0,max=100000))
phat = rbinom(m,n,p)
phat
m
n
p
phat = rbinom(m,n,p)
rbinom(m,n,p)
zsum.test(mean.x =235, sigma.x =75, n.x = 50)
install.packages("BSDA")
library(BSDA)
zsum.test(mean.x =235, sigma.x =75, n.x = 50)
tsum.test(mean.x =235, sigma.x =75, n.x = 50)
zsum.test(mean.x =235, sigma.x =75, n.x = 50)
zsum.test(mean.x =3.5, sigma.x =0.2, n.x = 50)
zsum.test(mean.x =3.5, sigma.x =0.8, n.x = 50)
weight=c(114, 100, 104, 94, 114, 105, 103, 105, 96)
t.test(weight)
std(weight)
st(weight)
sd(weight)
zsum.test(mean.x = mean(weight),sigma.x = sd(weight),n.x = 9)
install.packages("PASWR")
library(PASWR)
House
Price
qqnorm(House$Price)
qqline(House$Price)
qqline(c(1,2,3,4,5))
qqnorm(c(1,2,3,4,5))
qqnorm(c(1,2,3,4,5,4,3,2,1))
library(foreign)
data = read.dta("C:\\Users\\Administrator\\Desktop\\statistical computing\\lab09\\vacation.dta")
head(data)
attach(data)
names(data)
boxplot(miles~factor(kids),col = c(2,3,4,5,6,7))
income
library(UsingR)
simple.hist.and.boxplot(income)
t.test(income)
zsum.test(mean.x=90,sigma.x=10,n.x=49)$conf.int
a = zsum.test(mean.x=90,sigma.x=10,n.x=49)
a
a[1]
zsum.test(mean.x=90,sigma.x=10,n.x=49)$conf.int
?zsum.test
zsum.test(mean.x=90,sigma.x=10,n.x=49,conf.level = 0.9)$conf.int
head = scan()
t.test(head)$conf.int
qqnorm(head)
setwd("C:/Users/Administrator/Desktop/statistical computing/titanic")
